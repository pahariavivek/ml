{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "71bc4c22-a664-49e2-b9d2-e9d9f4bd5934",
    "_uuid": "6892f9c0163fe9044e5b1b2b86f1ea8eebfcb3af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1374978-25f9-4aa5-ab43-b0b38929a87c",
    "_uuid": "9fd1faedc5aa2ebca5123de54a5ac6202b50d61a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = pd.read_csv('data/mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1c98f932-bb07-41df-9ac6-ec76211bc611",
    "_uuid": "9636d427aebb4db0876399c4b52c67ff9f0aea67",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = images[images.columns[1:]].as_matrix()\n",
    "\n",
    "Y = np.zeros([len(images),10])\n",
    "for i in range(len(images)):\n",
    "   Y[i][images.iloc[i]['label']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e82ecded-38d4-4cc8-b093-6cf26be1509c",
    "_uuid": "980558dd3cc0e02b73948d76861645c013cf2477"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2ad7e024-76d8-4d42-ad05-982ba37da2d7",
    "_uuid": "5a1112b20a938472d19edc0a65937c56a1b4c49c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "samples = images.count()[0]\n",
    "\n",
    "epochs = 20 #increase this to get better accuracy\n",
    "starter_learning_rate = 0.01 #starting learning rate, decays exponentially\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "8c39b9f0-64cb-46f6-a79c-c537a013d982",
    "_uuid": "7e715a8a027a42d01cece84faaf65932b549549e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])  #[batchsize, 784]\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])   #[batchsize, 10]\n",
    "phase_train = tf.placeholder(tf.bool)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "abcb66bc-803d-4299-94da-9a59f517237e",
    "_uuid": "007cc64b1ec3612032a7172e21223f19f8cc7e4a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can skip this till model is fully created in below cells\n",
    "def batchnorm_layer(logits, n_out, conv=True):\n",
    "    \"\"\" Apply batch normalization before feeding into activation function. \n",
    "    During train phase simply calculate means & variance of given batch as well as keeps an\n",
    "    moving average of these means and variances for all the batches to be used during training\n",
    "    time.\n",
    "    Args:\n",
    "        logits: values before activation fun. If conv==True, dimension is \n",
    "        [batch, height, width, n_out] else [batch, n_out]\n",
    "        n_out: size of mean and variance vector\n",
    "        conv: Default True, if batch norm is applied to output of conv layer\n",
    "    Return:\n",
    "        outputs after applying batch normalization\n",
    "    \"\"\"\n",
    "    offset = tf.Variable(tf.constant(0.0, shape=[n_out]))\n",
    "    scale = tf.Variable(tf.constant(1.0, shape=[n_out]))\n",
    "        \n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.99)\n",
    "    if conv:\n",
    "        mean, variance = tf.nn.moments(logits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(logits, [0])\n",
    "\n",
    "    def mean_var_with_update():\n",
    "        update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "        with tf.control_dependencies([update_moving_avg]):\n",
    "            return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "    m, v = tf.cond(phase_train, mean_var_with_update\\\n",
    "                   , lambda: (exp_moving_avg.average(mean), exp_moving_avg.average(variance)))\n",
    "\n",
    "    Ybn = tf.nn.batch_normalization(logits, m, v, offset, scale, variance_epsilon=1e-5)\n",
    "    return Ybn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "292195ca-5e71-4e55-8ed8-43b1b36a593a",
    "_uuid": "735d1b4cfdce5b684c542d5968ce58136a4714c9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer 1\n",
    "#filter size = [5,5,1] = [height, width, input_channels] and 32 such filters i.e output_channels\n",
    "#stride = [1,1] with same padding\n",
    "w1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32])) #non zero bias\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])                        #[batch_size, 28, 28, 1]                      \n",
    "\n",
    "conv1 = tf.nn.conv2d(x_image, w1, [1, 1, 1, 1], padding=\"SAME\") #[batch_size, 28, 28, 32]\n",
    "bn1 = batchnorm_layer(tf.add(conv1, b1), 32)                    #[batch_size, 28, 28, 32]\n",
    "#a1 = tf.nn.relu(tf.add(conv1, b1))\n",
    "a1 = tf.nn.relu(bn1)                                            #[batch_size, 28, 28, 32]\n",
    "\n",
    "#max pooling filter size = [2,2]\n",
    "#max pooling stride = [2,2] with same padding\n",
    "pool1 = tf.nn.max_pool(a1, [1, 2, 2, 1], [1, 2, 2, 1], \"SAME\")  #[batch_size, 14, 14, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e981264e-7829-4382-b250-17a3bb95aa71",
    "_uuid": "e7819108a190ad101b1a7acaf903f6aa53c8a6bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer 2\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "conv2 = tf.nn.conv2d(pool1, w2, [1, 1, 1, 1], padding=\"SAME\")  #[batch_size, 14, 14, 64]\n",
    "bn2 = batchnorm_layer(tf.add(conv2, b2), 64)\n",
    "#a2 = tf.nn.relu(tf.add(conv2, b2))\n",
    "a2 = tf.nn.relu(bn2)\n",
    "\n",
    "pool2 = tf.nn.max_pool(a2, [1, 2, 2, 1], [1, 2, 2, 1], \"SAME\") #[batch_size, 7, 7, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0d6c3dce-afe4-424d-9567-696799995c33",
    "_uuid": "e817a016ffdf69d95840a058dae4a3023f8a9d8c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fully connectec layer\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "\n",
    "flat1 = tf.reshape(pool2, [-1, 7 * 7 * 64])                    #[batch_size, 3136]\n",
    "fc1 = tf.add(tf.matmul(flat1, w3), b3)                         #[batch_size, 1024]\n",
    "#a3 = tf.nn.relu(fc1)\n",
    "bn3 = batchnorm_layer(fc1, 1024, conv=False)\n",
    "a3 = tf.nn.relu(bn3)                                           #[batch_size, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "16525e87-abf9-459d-b72a-02afb8d66429",
    "_uuid": "53f13ab5ba6ed4b9dd1f79577437862fe796e181",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropout for regularization\n",
    "\n",
    "dropout = tf.nn.dropout(a3, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "9c3c7825-1063-4719-85e7-0a163ab70352",
    "_uuid": "ba04e8e01e8ed5c085d13d5e590e997b1b65c726",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output layer\n",
    "\n",
    "w4 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "pred = tf.matmul(dropout, w4) + b4                             #[batch_size, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfca5953-6c76-4675-94cd-707efa07b04f",
    "_uuid": "fcb067b6d5b95700dd0802ff4b54831d876abedc"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "740d52fa-2dbb-4946-8c2e-7709037872f7",
    "_uuid": "6fdadfd836699907c14356e573891a786466621e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, int(samples/batch_size), 0.96, staircase=True)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be145557-b8f1-4862-abf2-48b1b1263244",
    "_uuid": "336116604eebf54e113118e284370876a9050278",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inn = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(inn)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(samples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = X[i*batch_size:(i+1)*batch_size], Y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        #because of batch normalization which also helps in regularization using less max pooling\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: .20, phase_train: True})\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "print(\"Model has completed {} Epochs of Training\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35444b59-8868-4dbc-80e9-9616e954b709",
    "_uuid": "4bb036a26b322d560868c3b5940dea83b6814170"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb68bc68-88f1-415b-8e79-42d6f497f66f",
    "_uuid": "01f8b366507b767f1689b3df81aa7a0773570fcf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = pd.read_csv('data/mnist_test.csv')\n",
    "X_test = test_images.as_matrix()\n",
    "\n",
    "#no max pooling during test time\n",
    "w_value = sess.run(pred,feed_dict={x: X_test, keep_prob: 1.0, phase_train: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6988232c-8f23-405d-81d5-25df61a31f9a",
    "_uuid": "100c910ad0d06316193a5fddd89888f0dc650f6d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(w_value.argmax(axis=1), columns=[\"Label\"],index=range(1,28001))\n",
    "output.index.names = ['ImageId']\n",
    "output.to_csv('outputConvBN.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
