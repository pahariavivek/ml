{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy implementation of simple 2 layer nn\n",
    "\n",
    "first layer neurons are activated by relu followed by softmax in next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/simplenn_test.csv')\n",
    "train = pd.read_csv('data/simplenn_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train,test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### need to normalize training data as scale of features are diffrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_prc = data.drop([\"sample_no\",\"class_label\",\"class_label_binary\"],axis=1)\n",
    "\n",
    "SS = StandardScaler()\n",
    "SSModel = SS.fit(data_prc)\n",
    "sc_data = SSModel.transform(data_prc)\n",
    "train = sc_data\n",
    "target = data[\"class_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch size = 1\n",
    "def batch_data(X,Y):\n",
    "    y = Y.as_matrix() - 1\n",
    "    for i in range(X.shape[0]):\n",
    "        yield np.array(X[i]).T.reshape((178,1)), y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "output_size = 5\n",
    "learning_rate = 1e-1\n",
    "input_size = 178\n",
    "\n",
    "# model parameters\n",
    "W = np.random.randn(hidden_size, input_size)*0.1 # input to hidden\n",
    "b = np.zeros((hidden_size, 1)) # hidden bias\n",
    "Wy = np.random.randn(output_size, hidden_size)*0.1 # hidden to output\n",
    "by = np.zeros((output_size, 1)) # output bias\n",
    "\n",
    "def lossFunc(x, y):\n",
    "    #forward pass\n",
    "    h = np.dot(W, x) + b\n",
    "    ah = np.maximum(h,0)#relu\n",
    "    logits = np.dot(Wy, ah) + by\n",
    "    softmax = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    loss = - np.log(softmax[y])\n",
    "    #backward pass\n",
    "    dWy, dby, dW, db = np.zeros_like(Wy), np.zeros_like(by), np.zeros_like(W), np.zeros_like(b)\n",
    "    dy = np.copy(softmax)\n",
    "    dy[y] -= 1\n",
    "    dWy += np.dot(dy, ah.T)\n",
    "    dby += dy\n",
    "    dah = np.dot(Wy.T, dy)\n",
    "    dh = np.multiply((h > 0) * 1., dah)\n",
    "    dW += np.dot(dh, x.T)\n",
    "    db += dh\n",
    "    for dparam in [dWy, dby, dW, db]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    return loss, dWy, dby, dW, db\n",
    "\n",
    "smooth_loss = -np.log(1.0/output_size)\n",
    "mWy, mby, mW, mb = np.zeros_like(Wy), np.zeros_like(by), np.zeros_like(W), np.zeros_like(b)\n",
    "n = 0\n",
    "for epoch in range(20):\n",
    "    for x, y in batch_data(train,target):\n",
    "        loss, dWy, dby, dW, db = lossFunc(x, y)\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        if n % 100 == 0:\n",
    "            print(\"smooth loss {}\".format(smooth_loss)) # print progress\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wy, by, W, b], [dWy, dby, dW, db], [mWy, mby, mW, mb]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_proc = test.drop([\"sample_no\",\"class_label\",\"class_label_binary\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc_test = SSModel.transform(test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for x, y in batch_data(sc_test,test[\"class_label\"]):\n",
    "    h = np.dot(W, x) + b\n",
    "    ah = np.maximum(h,0)\n",
    "    logits = np.dot(Wy, ah) + by\n",
    "    pred = np.argmax(logits)\n",
    "    y_true.append(y)\n",
    "    y_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
